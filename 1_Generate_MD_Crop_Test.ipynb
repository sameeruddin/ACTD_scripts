{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the library installation for Mega Detector\n",
    "from utils.general import non_max_suppression, scale_coords, xyxy2xywh\n",
    "from utils.augmentations import letterbox\n",
    "import visualization.visualization_utils as viz_utils\n",
    "import numpy as np\n",
    "import ct_utils\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying the availability of the GPUs\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"PyTorch detected: {torch.cuda.get_device_name(0)} for processing.. \")\n",
    "    print(f\"Numbe of GPUs found: {torch.cuda.device_count()}\")\n",
    "    device = torch.device(\"cuda\") # if there are multiple GPUs assign the Device Id\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # clear the CUDA cache from the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f523e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "md_model_path = os.path.join(current_path, 'md_model', 'md_v5b.0.0.pt')\n",
    "input_image_path = os.path.join(current_path, 'Cat_Selfie.JPG')\n",
    "output_dir_path = os.path.join(current_path, 'Cat_Selfie_Crop')\n",
    "display(Image(filename=input_image_path, width=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables used by the megadetector model\n",
    "IMAGE_SIZE = 1280  # image size used in training\n",
    "STRIDE = 64\n",
    "DETECTION_THRESHOLD = 0.2\n",
    "# Label mapping for MegaDetector\n",
    "DEFAULT_DETECTOR_LABEL_MAP = ['animal', 'person', 'vehicle']\n",
    "# List variables to collect the output\n",
    "final_output = {'input_file' : f'{os.path.basename(input_image_path)}'}\n",
    "tot_detections = []\n",
    "labels = []\n",
    "bbox = []\n",
    "scores = []\n",
    "max_conf = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading a model onto the {device} for inference analysis.. ')\n",
    "try:\n",
    "    model_checkpoint = torch.load(md_model_path, device)\n",
    "    model = model_checkpoint['model'].float().fuse().eval()\n",
    "    print(f'[info]: Model is loaded to the {device}')\n",
    "except OSError as oer:\n",
    "    print(oer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7373db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Upsample):\n",
    "        m.recompute_scale_factor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a34d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pre-processing {os.path.basename(input_image_path)} file.. ')\n",
    "# img_data = viz_utils.load_image(input_image_path) # Load the image file\n",
    "img_data = cv2.imread(input_image_path) # read the image using cv2 library\n",
    "cpy_img_data = img_data\n",
    "img_array = np.asarray(img_data) # Convert the image data into a numpy array\n",
    "img_resize = letterbox(img_array, new_shape=IMAGE_SIZE, stride=STRIDE, auto=True)[0] # Resize the image shape as per the megadetector model\n",
    "img_transpose = img_resize.transpose((2, 0, 1))  # HWC to CHW; PIL Image is RGB already\n",
    "img = np.ascontiguousarray(img_transpose)\n",
    "img = torch.from_numpy(img)\n",
    "img = img.to(device)\n",
    "img = img.float()\n",
    "img /= 255\n",
    "if len(img.shape) == 3:  # always true for now, TODO add inference using larger batch size\n",
    "    img = torch.unsqueeze(img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29199ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mega detector look for objects in the image.. ')\n",
    "with torch.no_grad():\n",
    "    result = model(img)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Apply non-maximum supression to the results of Mega Detector... ')\n",
    "result_nms = non_max_suppression(prediction=result, conf_thres=DETECTION_THRESHOLD) # applying non-maximum supression logic to the results\n",
    "normalization = torch.tensor(img_array.shape)[[1, 0, 1, 0]]  # normalization gain whwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the feature from the object detected\n",
    "for detection in result_nms:\n",
    "    if len(detection):\n",
    "        detection[:, :4] = scale_coords(img.shape[2:], detection[:, :4], img_array.shape).round()\n",
    "        for *xyxy, conf, cls in reversed(detection):\n",
    "            # normalized center-x, center-y, width and height\n",
    "            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / normalization).view(-1).tolist()\n",
    "            api_box = ct_utils.convert_yolo_to_xywh(xywh)\n",
    "            conf = ct_utils.truncate_float(conf.tolist(), precision=3)\n",
    "            # MegaDetector output format's categories start at 1, but this model's start at 0\n",
    "            cls = int(cls.tolist())\n",
    "            labels.append(DEFAULT_DETECTOR_LABEL_MAP[cls])\n",
    "            scores.append(conf)\n",
    "            bbox.append(ct_utils.truncate_float_array(api_box, precision=4))\n",
    "            tot_detections.append({ 'category': cls, 'conf': conf, 'bbox': ct_utils.truncate_float_array(api_box, precision=4)})\n",
    "            max_conf = max(max_conf, conf)\n",
    "final_output['max_detection_conf'] = max_conf\n",
    "final_output['detections'] = tot_detections\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9356c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop the region of interest of an image using the bounding box predicted by the mega detector\n",
    "# select the bounding box that has highest confidence score\n",
    "for detection_values in final_output['detections']:\n",
    "    if detection_values['conf'] == final_output['max_detection_conf']:\n",
    "        bboxes = detection_values['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the format of the bounding boxes are in x1,y1,w_box, h_box -> defined in visualization_utils.py\n",
    "x1,y1,w_box,h_box = bboxes[0], bboxes[1], bboxes[2], bboxes[3]\n",
    "\n",
    "# convert the bbox co-ordinates\n",
    "ymin,xmin,ymax,xmax = y1, x1, y1 + h_box, x1 + w_box\n",
    "\n",
    "#find the width and height of the image using cv2\n",
    "im_height, im_width = img_data.shape[0], img_data.shape[1]\n",
    "\n",
    "# adjust the bounding box co-ordinates with the image dimensions to crop the region of interest\n",
    "(left, right, top, bottom) = (int(xmin * im_width), int(xmax * im_width), int(ymin * im_height), int(ymax * im_height))\n",
    "\n",
    "# loading the bounding box xo-ordinates information for extracting RoI\n",
    "roi_image = cpy_img_data[top:bottom, left:right]\n",
    "\n",
    "# extracting input filename\n",
    "head,tail = (os.path.basename(input_image_path)).split('.')\n",
    "\n",
    "# building output file path to save an image\n",
    "output_image_path = os.path.join(output_dir_path, f'{head}.JPG')\n",
    "\n",
    "# saving the RoI part as a separate image\n",
    "cv2.imwrite(output_image_path, roi_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439236a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
